{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, balanced_accuracy_score, recall_score, f1_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import numpy as np\n",
    "import gzip\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import zipfile\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df_entrenamiento = pd.read_csv('../files/input/train_data.csv/train_default_of_credit_card_clients.csv')\n",
    "df_prueba = pd.read_csv('../files/input/test_data.csv/test_default_of_credit_card_clients.csv')\n",
    "\n",
    "# Renombrar la columna \"default payment next month\" a \"default\"\n",
    "df_entrenamiento.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "df_prueba.rename(columns={'default payment next month': 'default'}, inplace=True)\n",
    "\n",
    "# Remover la columna \"ID\"\n",
    "df_entrenamiento.drop(columns=['ID'], inplace=True)\n",
    "df_prueba.drop(columns=['ID'], inplace=True)\n",
    "\n",
    "# Eliminar registros con información no disponible\n",
    "df_entrenamiento.dropna(inplace=True)\n",
    "df_prueba.dropna(inplace=True)\n",
    "\n",
    "# Agrupar valores de EDUCATION > 4 en la categoría \"others\"\n",
    "df_entrenamiento['EDUCATION'] = df_entrenamiento['EDUCATION'].apply(lambda x: 4 if x > 4 else x)\n",
    "df_prueba['EDUCATION'] = df_prueba['EDUCATION'].apply(lambda x: 4 if x > 4 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en características (X) y etiquetas (y)\n",
    "x_entrenamiento = df_entrenamiento.drop(columns=['default'])\n",
    "y_entrenamiento = df_entrenamiento['default']\n",
    "x_prueba = df_prueba.drop(columns=['default'])\n",
    "y_prueba = df_prueba['default']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas categóricas y numéricas\n",
    "columnas_categoricas = ['SEX', 'EDUCATION', 'MARRIAGE']\n",
    "columnas_numericas = [col for col in x_entrenamiento.columns if col not in columnas_categoricas]\n",
    "\n",
    "# Preprocesamiento para variables categóricas y numéricas\n",
    "preprocesador = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(), columnas_categoricas),\n",
    "        ('num', 'passthrough', columnas_numericas)\n",
    "    ])\n",
    "\n",
    "# Crear el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocesador', preprocesador),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('escalador', MinMaxScaler()),\n",
    "    ('selector', SelectKBest(score_func=f_regression)),\n",
    "    ('clasificador', MLPClassifier(max_iter=30000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a optimizar\n",
    "parametros_grid = {\n",
    "    'selector__k': range(1, len(x_entrenamiento.columns) + 1),\n",
    "    'clasificador__hidden_layer_sizes': [(n,) for n in range(1, 11)],\n",
    "    'clasificador__solver': ['adam'],\n",
    "    'clasificador__learning_rate_init': [0.01, 0.001, 0.0001],\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "busqueda_grid = GridSearchCV(pipeline, parametros_grid, cv=10, scoring='balanced_accuracy', refit=True, verbose=2)\n",
    "busqueda_grid.fit(x_entrenamiento, y_entrenamiento)\n",
    "\n",
    "# Mejor modelo encontrado\n",
    "mejor_modelo = busqueda_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo comprimido\n",
    "with gzip.open('../files/models/model.pkl.gz', 'wb') as f:\n",
    "    pickle.dump(mejor_modelo, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en los conjuntos de entrenamiento y prueba\n",
    "y_entrenamiento_pred = mejor_modelo.predict(x_entrenamiento)\n",
    "y_prueba_pred = mejor_modelo.predict(x_prueba)\n",
    "\n",
    "# Calcular métricas para el conjunto de entrenamiento\n",
    "metricas_entrenamiento = {\n",
    "    'type': 'metrics',\n",
    "    'dataset': 'train',\n",
    "    'precision': precision_score(y_entrenamiento, y_entrenamiento_pred),\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_entrenamiento, y_entrenamiento_pred),\n",
    "    'recall': recall_score(y_entrenamiento, y_entrenamiento_pred),\n",
    "    'f1_score': f1_score(y_entrenamiento, y_entrenamiento_pred)\n",
    "}\n",
    "\n",
    "# Calcular métricas para el conjunto de prueba\n",
    "metricas_prueba = {\n",
    "    'type': 'metrics',\n",
    "    'dataset': 'test',\n",
    "    'precision': precision_score(y_prueba, y_prueba_pred),\n",
    "    'balanced_accuracy': balanced_accuracy_score(y_prueba, y_prueba_pred),\n",
    "    'recall': recall_score(y_prueba, y_prueba_pred),\n",
    "    'f1_score': f1_score(y_prueba, y_prueba_pred)\n",
    "}\n",
    "\n",
    "# Calcular la matriz de confusión para el conjunto de entrenamiento\n",
    "mc_entrenamiento = confusion_matrix(y_entrenamiento, y_entrenamiento_pred)\n",
    "mc_entrenamiento_dict = {\n",
    "    'type': 'cm_matrix',\n",
    "    'dataset': 'train',\n",
    "    'true_0': {'predicted_0': int(mc_entrenamiento[0, 0]), 'predicted_1': int(mc_entrenamiento[0, 1])},\n",
    "    'true_1': {'predicted_0': int(mc_entrenamiento[1, 0]), 'predicted_1': int(mc_entrenamiento[1, 1])}\n",
    "}\n",
    "\n",
    "# Calcular la matriz de confusión para el conjunto de prueba\n",
    "mc_prueba = confusion_matrix(y_prueba, y_prueba_pred)\n",
    "mc_prueba_dict = {\n",
    "    'type': 'cm_matrix',\n",
    "    'dataset': 'test',\n",
    "    'true_0': {'predicted_0': int(mc_prueba[0, 0]), 'predicted_1': int(mc_prueba[0, 1])},\n",
    "    'true_1': {'predicted_0': int(mc_prueba[1, 0]), 'predicted_1': int(mc_prueba[1, 1])}\n",
    "}\n",
    "\n",
    "# Guardar las métricas y matrices de confusión en un archivo JSON\n",
    "with open(\"../files/output/metrics.json\", \"w\") as f:\n",
    "    json.dump(metricas_entrenamiento, f)\n",
    "    f.write('\\n')  \n",
    "    json.dump(metricas_prueba, f)\n",
    "    f.write('\\n')  \n",
    "    json.dump(mc_entrenamiento_dict, f)\n",
    "    f.write('\\n')  \n",
    "    json.dump(mc_prueba_dict, f)\n",
    "    f.write('\\n')\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
